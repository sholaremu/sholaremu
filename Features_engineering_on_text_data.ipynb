{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Features engineering on text data.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNwRdVnvZX1HHjjNUq8Qfxf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sholaremu/sholaremu/blob/main/Features_engineering_on_text_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oo7Oo9fB6Fv9"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arRRvGBjs5IG"
      },
      "source": [
        "corpus = ['The sky is blue and beautiful.',\n",
        "          'Love this blue and beautiful sky!',\n",
        "          'The quick brown fox jumps over the lazy dog.',\n",
        "          'The brown fox is quick and the blue dog is lazy!',\n",
        "          'The sky is very blue and the sky is very beautiful today',\n",
        "          'The dog is lazy but  the brown fox is quick!'\n",
        "]\n",
        "labels = ['weather', 'weather', 'animals', 'animals', 'weather', 'animals']\n",
        "corpus = np.array(corpus)\n",
        "corpus_df = pd.DataFrame({'Document': corpus, \n",
        "                          'Category': labels})\n",
        "corpus_df = corpus_df[['Document', 'Category']]\n",
        "corpus_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoYHoMr31wlD"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LY-vb0UXy-Rt"
      },
      "source": [
        "wpt = nltk.WordPunctTokenizer()\n",
        "stop_words = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "def normalize_document(doc):\n",
        "  # lower case and remove special characters\\whitespaces\n",
        "  doc = re.sub(r'[^a-zA-Z0-9\\s]', '', doc, re.I)\n",
        "  doc = doc.lower()\n",
        "  doc = doc.strip()\n",
        "\n",
        "  # tokenize document\n",
        "  tokens = wpt.tokenize(doc)\n",
        "  # filter stopwords out of document\n",
        "  filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "  # re-create document from filtered tokens\n",
        "  doc = ' '.join(filtered_tokens)\n",
        "  return doc\n",
        "\n",
        "normalize_corpus = np.vectorize(normalize_document)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-80bp712J7G"
      },
      "source": [
        "norm_corpus = normalize_corpus(corpus)\n",
        "norm_corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRUxf7Yx3gtN"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "cv = CountVectorizer(min_df=0., max_df=1.)\n",
        "cv_matrix = cv.fit_transform(norm_corpus)\n",
        "cv_matrix = cv_matrix.toarray()\n",
        "cv_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "453-haR-5cpW"
      },
      "source": [
        "vocab = cv.get_feature_names()\n",
        "pd.DataFrame(cv_matrix, columns=vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hpq3k_kl6iK1"
      },
      "source": [
        "bv = CountVectorizer(ngram_range=(3,3))\n",
        "bv_matrix = bv.fit_transform(norm_corpus)\n",
        "bv_matrix = bv_matrix.toarray()\n",
        "vocab = bv.get_feature_names()\n",
        "pd.DataFrame(bv_matrix, columns=vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56LddArg9plh"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tv = TfidfVectorizer(min_df=0., max_df=1., use_idf=True)\n",
        "tv_matrix = tv.fit_transform(norm_corpus)\n",
        "tv_matrix = tv_matrix.toarray()\n",
        "\n",
        "vocab = tv.get_feature_names()\n",
        "pd.DataFrame(np.round(tv_matrix, 2), columns=vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4FpU8jGC8Zk"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "similarity_matrix = cosine_similarity(tv_matrix)\n",
        "similarity_df = pd.DataFrame(similarity_matrix)\n",
        "similarity_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IX8ko4mLFa9K"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "km = KMeans(n_clusters=2)\n",
        "km.fit_transform(similarity_df)\n",
        "cluster_labels = km.labels_\n",
        "cluster_labels = pd.DataFrame(cluster_labels, columns=['ClusterLabel'])\n",
        "pd.concat([corpus_df, cluster_labels], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjffLgu2JwSO"
      },
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "lda = LatentDirichletAllocation(n_components=2, max_iter=100, random_state=42)\n",
        "dt_matrix = lda.fit_transform(tv_matrix)\n",
        "features = pd.DataFrame(dt_matrix, columns=['T1', 'T2'])\n",
        "features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ulI0dW_L4Wv"
      },
      "source": [
        "tt_matrix = lda.components_\n",
        "for topic_weights in tt_matrix:\n",
        "  topic = [(token, weight) for token, weight in zip(vocab, topic_weights)]\n",
        "  topic = sorted(topic, key=lambda x: -x[1])\n",
        "  topic = [item for item in topic if item[1] > 0.6]\n",
        "  print(topic)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzWvWjEoYjH8"
      },
      "source": [
        "m = KMeans(n_clusters=2)\n",
        "km.fit_transform(features)\n",
        "cluster_labels =km.labels_\n",
        "cluster_labels =pd.DataFrame(cluster_labels, columns=['ClusterLabels'])\n",
        "pd.concat([corpus_df, cluster_labels], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXT1QQ6ea7wK"
      },
      "source": [
        "from gensim.models import word2vec\n",
        "\n",
        "wpt = nltk.WordPunctTokenizer()\n",
        "tokenized_corpus = [wpt.tokenize(document) for document in norm_corpus]\n",
        "\n",
        "    # Set values for various parameters\n",
        "feature_size = 10 # Word vector dimensionality\n",
        "window_context = 10 # Context window size\n",
        "min_word_count = 1 # Minimum word count\n",
        "sample =1e-3 # Downsamle setting for frequent words\n",
        "\n",
        "w2v_model = word2vec.Word2Vec(tokenized_corpus, size=feature_size,\n",
        "                                  window=window_context, min_count= min_word_count, sample=sample)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-MIrdXzeGjL"
      },
      "source": [
        "w2v_model.wv['sky']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qE6HL5NCfANH"
      },
      "source": [
        "def average_word_vectors(words, model, vocabulary, num_features):\n",
        "\n",
        "  feature_vector = np.zeros((num_features,), dtype=\"float64\")\n",
        "  nwords =0.\n",
        "\n",
        "  for word in words:\n",
        "    if word in vocabulary:\n",
        "      nwords = nwords + 1.\n",
        "\n",
        "  feature_vector = np.add(feature_vector, model[word])\n",
        "\n",
        "  if nwords:\n",
        "    feature_vector = np.divide(feature_vector, nwords)\n",
        "\n",
        "\n",
        "    return feature_vector\n",
        "\n",
        "def averaged_word_vectorizer(corpus, model, num_features):\n",
        "      vocabulary = set(model.wv.index2word)\n",
        "      features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
        "      \n",
        "      for tokenized_sentence in corpus]\n",
        "      return np.array(features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGeMjdJrkFVv"
      },
      "source": [
        "w2v_feature_array = averaged_word_vectorizer(corpus=tokenized_corpus, model=w2v_model, num_features=feature_size)\n",
        "pd.DataFrame(w2v_feature_array)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwICNKEdpVwA"
      },
      "source": [
        "from sklearn.cluster import AffinityPropagation\n",
        "\n",
        "ap = AffinityPropagation()\n",
        "ap.fit(w2v_feature_array)\n",
        "cluster_labels = ap.labels_\n",
        "cluster_labels = pd.DataFrame(cluster_labels, columns=['ClusterLabel'])\n",
        "pd.concat([corpus_df, cluster_labels], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}